\documentclass{article}
\author{Abraham Levkoy}
\title{SICP exercises, section 2.3}

\usepackage{mystyle}

\begin{document}
\maketitle

\section{Exercise 2.53}
\begin{quote}
    What would the interpreter print in response to evaluating each of the
    following expressions?
\end{quote}

\begin{lstlisting}
(list 'a 'b 'c)
> (a b c)
(list (list 'george))
> ((george))
(cdr '((x1 x2) (y1 y2)))
> ((y1 y2))
(cadr '((x1 x2) (y1 y2)))
> (y1 y2)
(pair? (car '(a short list)))
> #f
(memq 'red '((red shoes) (blue socks)))
> #f
(memq 'red '(red shoes blue socks))
> #t
\end{lstlisting}

\section{Exercise 2.54}
\begin{quote}
    Two lists are said to be \texttt{equal?} if they contain equal elements
    arranged in the same order. For example,

    \begin{lstlisting}
(equal? '(this is a list)
        '(this is a list))
    \end{lstlisting}

is true, but

    \begin{lstlisting}
(equal? '(this is a list)
        '(this (is a) list))
    \end{lstlisting}

    is false. To be more precise, we can define \texttt{equal?} recursively in
    terms of the basic \texttt{eq?} equality of symbols by saying that
    \texttt{a} and \texttt{b} are \texttt{equal?} if they are both symbols and
    the symbols are \texttt{eq?}, or if they are both lists such that
    \texttt{(car a)} is \texttt{equal?} to \texttt{(car b)} and \texttt{(cdr
    a)} is \texttt{equal?} to \texttt{(cdr b)}. Using this idea, implement
    \texttt{equal?} as a procedure.
\end{quote}

\lstinputlisting[firstline=3,lastline=9]{ch2/ex2.54.scm}

\section{Exercise 2.55}
\begin{quote}
    Eva Lu Ator types to the interpreter the expression

    \begin{lstlisting}
(car ''abracadabra)
    \end{lstlisting}

    To her surprise, the interpreter prints back \texttt{quote}. Explain.
\end{quote}

The character \texttt{'} is a syntactic shorthand for using the special form
\texttt{quote} with the following object as the argument. It evaluates to a
list of the symbols in the argument. When it is used on itself, as here, the
inner \texttt{quote} gets evaluated by the outer \texttt{quote}, producing a
list with \texttt{quote} as its first element, i.e.
\begin{lstlisting}
(quote abracadabra)
\end{lstlisting}
\texttt{car} then evaluates to the first element of its argument.

\section{Exercise 2.56}
\begin{quote}
    Show how to extend the basic differentiator to handle more kinds of
    expressions. For instance, implement the differentiation rule
    $$\frac{d(u^n)}{dx} = nu^{n-1}\frac{du}{dx}$$
    by adding a new clause to the \texttt{deriv} program and defining
    appropriate procedures \texttt{exponentiation?}, \texttt{base},
    \texttt{exponent}, and \texttt{make-exponentiation}. (You may use the
    symbol \texttt{**} to denote exponentiation.) Build in the rules that
    anything raised to the power 0 is 1 and anything raised to the power 1 is
    the thing itself.
\end{quote}

\lstinputlisting[firstline=47,lastline=60]{ch2/ex2.56.scm}
\lstinputlisting[firstline=63,lastline=87]{ch2/ex2.56.scm}

\section{Exercise 2.57}
\begin{quote}
    Extend the differentiation program to handle sums and products of arbitrary
    numbers of (two or more) terms. Then the last example above could be
    expressed as
    \begin{lstlisting}
(deriv '(* x y (+ x 3)) 'x)
    \end{lstlisting}
    Try to do this by changing only the representation for sums and products,
    without changing the \texttt{deriv} procedure at all. For example, the
    \texttt{addend} of a sum would be the first term, and the \texttt{augend}
    would be the sum of the rest of the terms.
\end{quote}

The new representation of sums and products is a list with the operator in the
first position and the terms in subsequent positions. The only things that needs
to change are the selectors for the second operands:
\lstinputlisting[firstline=111,lastline=123]{ch2/ex2.56.scm}

\section{Exericse 2.58}
\begin{quote}
    Suppose we want to modify the differentiation program so that it works with
    ordinary mathematical notation, in which \texttt{+} and \texttt{*} are
    infix rather than prefix operators. Since the differentiation program is
    defined in terms of abstract data, we can modify it to work with different
    representations of expressions solely by changing the predicates,
    selectors, and constructors that define the representation of the algebraic
    expressions on which the differentiator is to operate.
    \begin{enumerate}
        \item Show how to do this in order to differentiate algebraic
            expressions presented in infix form, such as \texttt{(x + (3 * (x +
            (y + 2))))}. To simplify the task, assume that \texttt{+} and
            \texttt{*} always take two arguments and that expressions are fully
            parenthesized.
        \item The problem becomes substantially harder if we allow standard
            algebraic notation, such as \texttt{(x + 3 * (x + y + 2))}, which
            drops unnecessary parentheses and assumes that multiplication is
            done before addition. Can you design appropriate predicates,
            selectors, and constructors for this notation such that our
            derivative program still works?
    \end{enumerate}
\end{quote}

\begin{enumerate}
    \item In fully parenthesized infix form, the only things that need to
        change are the constructors and selectors for the different types of
        expressions.
        \lstinputlisting[firstline=147,lastline=200]{ch2/ex2.56.scm}
    \item Using standard algebraic notation, the predicates identifying the
        different types of expressions need to employ a notion of precedence.
        An expression is only of the type indicated by a certain operator if no
        lower-precedence operators are present. The selectors are modified to
        search for the operand relevant to their type and split the overall
        expression around that operator. The constructors stay the same as in
        part 1, which can result in output expressions that are not reduced to
        the simplest form, but that would be considerably more complicated.
        \lstinputlisting[firstline=211,lastline=288]{ch2/ex2.56.scm}
\end{enumerate}

\section{Exercise 2.59}
\begin{quote}
    Implement the \texttt{union-set} operation for the unordered-list
    representation of sets.
\end{quote}

\lstinputlisting[firstline=24,lastline=29]{ch2/ex2.59.scm}

This is the same approach as \texttt{intersection-set}, \texttt{car}-ing down
\texttt{set1} and \texttt{cons}-ing up \texttt{set2}. This has the disadvantage
that, each time we search in \texttt{set2} for an element from \texttt{set1},
we needlessly consider all of the elements that we have already added from
\texttt{set1}. If \texttt{set1} has size $M$, and \texttt{set2} has size $N$,
this produces a worst-case (sets totally disjoint) runtime complexity of
\begin{equation*}
    O(M * N + \sum_{i=1}^{M}i = O(M * N + \frac{M^2}{2}) = O(M^2 + MN).
\end{equation*}

An alternative strategy would be to store the non-intersecting elements of
\texttt{set1} in a third set, \texttt{set3} then append \texttt{set2} to that
set. This would only pay the cost of traversing the non-intersecting elements
once:
\begin{equation*}
    O(M * N + M) = O(M *N)
\end{equation*}

If $M \approx N$, these asymptotic complexities are the same. Consequently, it
does not seem worth the additional code complexity to optimize this algorithm,
especially considering the inefficiency of the underlying data structure. There
are lower-hanging fruit here.

\section{Exercise 2.60}
\begin{quote}
    We specified that a set would be represented as a list with no duplicates.
    Now suppose we allow duplicates. For instance, the set ${1,2,3}$ could
    be represented as the list \texttt{(2 3 2 1 3 2 2)}. Design procedures
    \texttt{element-of-set?}, \texttt{adjoin-set}, \texttt{union-set}, and
    \texttt{intersection-set} that operate
    on this representation. How does the efficiency of each compare with the
    corresponding procedure for the non-duplicate representation? Are there
    applications for which you would use this representation in preference to
    the non-duplicate one?
\end{quote}

\texttt{element-of-set?} and \texttt{intersection-set} remain the same. Because
sets do not need to be deduplicated, \texttt{adjoin-set} and \texttt{union-set}
can be simplified.

\lstinputlisting[firstline=9,lastline=10]{ch2/ex2.60.scm}
\lstinputlisting[firstline=22,lastline=23]{ch2/ex2.60.scm}

The time complexity of \texttt{element-of-set?} remains $O(N)$, but $N$ is now
the sizes of the list representing \texttt{set}, not the numbers of distinct
elements in \texttt{set}. Likewise, the complexity of \texttt{intersection-set}
is still $O(NM)$, but $N$ and $M$ are now the size of the lists representing
\texttt{set1} and \texttt{set2}, not the number of distinct elements in those
sets.

\texttt{adjoin-set} now has time complexity $O(1)$, as compared to $O(N)$, and
\texttt{union-set} has time complexity $O(N+M)$ (referring to the sizes of the
representations), as compared to $O(M^2 + MN)$ (referring to the numbers of
distinct elements in the sets).

The sets themselves now take up more memory than in the previous representation.
The memory cost varies according to the degree of duplication in the sets.

This representation would be preferable to the deduplicated representation if
\texttt{adjoin-set} and \texttt{union-set} operations were frequent, and
\texttt{element-of-set?} and \texttt{intersection-set} operations were
infrequent, assuming that memory is plentiful enough to easily accomodate the
expected degree of duplication. It would also be preferable in most scenarios
where the average degree of duplication was expected to be low. In terms of
asymptotic complexity, the representation allowing duplicates would be more
efficient as long as the number of duplicates in a typical set was
$O(\textrm{number of distinct elements})$.

\section{Exercise 2.61}
\begin{quote}
    Give an implementation of \texttt{adjoin-set} using the ordered
    representation. By analogy with \texttt{element-of-set?} show how to take
    advantage of the ordering to produce a procedure that requires on the
    average about half as many steps as with the unordered representation.
\end{quote}

\lstinputlisting[firstline=25,lastline=35]{ch2/ex2.61.scm}

This version does use $O(N)$ stack space, unlike the previous version, but it
is already returning an $N$-size list, so the space complexity doesn't change.

\section{Exercise 2.62}
\begin{quote}
    Give a $\Theta(n)$ implementation of \texttt{union-set} for sets
    represented as ordered lists.
\end{quote}

This follows the same general form as \texttt{intersection-set}.
\lstinputlisting[firstline=57,lastline=71]{ch2/ex2.61.scm}

\section{Exercise 2.63}
\begin{quote}
    Each of the following two procedures converts a binary tree to a list.

    \begin{lstlisting}
(define (tree->list-1 tree)
  (if (null? tree)
      '()
      (append
       (tree->list-1
        (left-branch tree))
       (cons (entry tree)
             (tree->list-1
              (right-branch tree))))))

(define (tree->list-2 tree)
  (define (copy-to-list tree result-list)
    (if (null? tree)
        result-list
        (copy-to-list
         (left-branch tree)
         (cons (entry tree)
               (copy-to-list
                (right-branch tree)
                result-list)))))
  (copy-to-list tree '()))
    \end{lstlisting}

    \begin{enumerate}
        \item Do the two procedures produce the same result for every tree? If
            not, how do the results differ? What lists do the two procedures
            produce for the trees in Figure 2.16?
        \item Do the two procedures have the same order of growth in the number
            of steps required to convert a balanced tree with $n$ elements to a
            list? If not, which one grows more slowly?
    \end{enumerate}
\end{quote}

\begin{enumerate}
    \item Both procedures have the effect of outputting the in-order traversal
        of the input tree. Consequently, they produce the same result for every
        tree. For all of the lists in Figure 2.16, both procedures
        produce the list \texttt{(1 3 5 7 9)}.
    \item Both procedures traverse each element of the tree once.
        \texttt{tree->list-2} just performs a constant-time \texttt{cons}
        operation for each element, so \texttt{tree->list-2} has $O(n)$ time
        complexity.

        \texttt{tree->list-1} performs an \texttt{append} for each
        tree element. \texttt{Append} has time complexity linear in the number
        of elements in the first argument, which is the list form of the left
        branch of the current element. Because the tree is balanced, each
        recursive invocation of \texttt{tree->list-1} calls \texttt{append} with
        a list approximately half as long as in the previous invocation of
        \texttt{tree->list-1}. Therefore, the time complexity of
        \texttt{tree->list-1} is $O(n\log n)$.

        The number of steps for \texttt{tree->list-2} grows more slowly than for
        \texttt{tree->list-1}.
\end{enumerate}

\section{Exericse 2.64}
\begin{quote}
    The following procedure \texttt{list->tree} converts an ordered list to a
    balanced binary tree. The helper procedure \texttt{partial-tree} takes as
    arguments an integer $n$ and list of at least $n$ elements and constructs a
    balanced tree containing the first $n$ elements of the list. The result
    returned by \texttt{partial-tree} is a pair (formed with \texttt{cons})
    whose \texttt{car} is the constructed tree and whose \texttt{cdr} is the
    list of elements not included in the tree.

    \begin{lstlisting}
(define (list->tree elements)
  (car (partial-tree
        elements (length elements))))

(define (partial-tree elts n)
  (if (= n 0)
      (cons '() elts)
      (let ((left-size
             (quotient (- n 1) 2)))
        (let ((left-result
               (partial-tree
                elts left-size)))
          (let ((left-tree
                 (car left-result))
                (non-left-elts
                 (cdr left-result))
                (right-size
                 (- n (+ left-size 1))))
            (let ((this-entry
                   (car non-left-elts))
                  (right-result
                   (partial-tree
                    (cdr non-left-elts)
                    right-size)))
              (let ((right-tree
                     (car right-result))
                    (remaining-elts
                     (cdr right-result)))
                (cons (make-tree this-entry
                                 left-tree
                                 right-tree)
                      remaining-elts))))))))
    \end{lstlisting}

    \begin{enumerate}
        \item Write a short paragraph explaining as clearly as you can how
            \texttt{partial-tree} works. Draw the tree produced by
            \texttt{list->tree} for the list \texttt{(1 3 5 7 9 11)}.
        \item What is the order of growth in the number of steps required by
            \texttt{list->tree} to convert a list of $n$ elements?
    \end{enumerate}
\end{quote}

\begin{enumerate}
    \item \texttt{partial-tree} constructs a tree from a list recursively,
        using the element in the approximate middle of the list as the root and
        trees constructed from the list elements to the left and right of that
        as the left and right branches, respectively. At each level of
        recursion, the number of list elements under consideration is halved.
        When 0 elements are under consideration, an empty tree is produced, so
        1 list element produces a tree with a root and empty branches. By
        keeping track of the elements that haven't been added to the tree yet,
        \texttt{partial-tree} moves through the list from left to right
        (roughly), adding elements to the tree as it goes until there are none
        left to add.

        \begin{allintypewriter}

            \Tree [.5 [.1 \edge[draw=none]; {} 3 ]
                      [.9 7 11 ]]

        \end{allintypewriter}

    \item \texttt{lsit->tree} performs $O(n)$ steps for an $n$-element list. At
        each level of recursion, it performs one $O(1)$ step (constructing the
        tree to return) and two recursive calls with input size $n/2$. This
        means a call with input-size $n$ takes twice as long as a call with
        input size $n/2$, so the time complexity is linear.
\end{enumerate}

\section{Exercise 2.65}
\begin{quote}
    Use the results of Exercise 2.63 and Exercise 2.64 to give $\Theta(n)$
    implementations of \texttt{union-set} and \texttt{intersection-set} for
    sets implemented as (balanced) binary trees.
\end{quote}

These both consist of transforming the input trees into sorted lists, performing
the specified operation on the sorted lists, and transforming the result back
into a sorted, balanced tree. Each of these operations are linear in the number
of elements in the input trees, so the combination is also $\Theta(n)$.

\lstinputlisting[firstline=152,lastline=171]{ch2/ex2.63.scm}

\lstinputlisting[firstline=197,lastline=215]{ch2/ex2.63.scm}

\section{Exercise 2.66}
\begin{quote}
    Implement the \texttt{lookup} procedure for the case where the set of
    records is structured as a binary tree, ordered by the numerical values of
    the keys.
\end{quote}

This works basically the same way as \texttt{element-of-set?} for the tree
representation with the added complication that the tree is ordered based on
the keys of the records.

\lstinputlisting[firstline=10,lastline=26]{ch2/ex2.66.scm}

\section{Exercise 2.67}
\begin{quote}
    Define an encoding tree and a sample message:

    \begin{lstlisting}
(define sample-tree
  (make-code-tree
   (make-leaf 'A 4)
   (make-code-tree
    (make-leaf 'B 2)
    (make-code-tree
     (make-leaf 'D 1)
     (make-leaf 'C 1)))))

(define sample-message
  '(0 1 1 0 0 1 0 1 0 1 1 1 0))
    \end{lstlisting}

    Use the \texttt{decode} procedure to decode the message, and give the
    result.
\end{quote}

The decoded message is \texttt{(A D A B B C A)}.

\section{Exercise 2.68}
\begin{quote}
    The \texttt{encode} procedure takes as arguments a message and a tree and
    produces the list of bits that gives the encoded message.

    \begin{lstlisting}
(define (encode message tree)
  (if (null? message)
      '()
      (append
       (encode-symbol (car message)
                      tree)
       (encode (cdr message) tree))))
    \end{lstlisting}

    \texttt{Encode-symbol} is a procedure, which you must write, that returns
    the list of bits that encodes a given symbol according to a given tree. You
    should design \texttt{encode-symbol} so that it signals an error if the
    symbol is not in the tree at all. Test your procedure by encoding the
    result you obtained in Exercise 2.67 with the sample tree and seeing
    whether it is the same as the original sample message.
\end{quote}

\lstinputlisting[firstline=110,lastline=125]{ch2/ex2.67.scm}

This program produces the original encoded message from the decoded message.

\section{Exercise 2.69}
\begin{quote}
    The following procedure takes as its argument a list of symbol-frequency
    pairs (where no symbol appears in more than one pair) and generates a
    Huffman encoding tree according to the Huffman algorithm.

    \begin{lstlisting}
(define (generate-huffman-tree pairs)
  (successive-merge
   (make-leaf-set pairs)))
    \end{lstlisting}

    \texttt{Make-leaf-set} is the procedure given above that transforms the
    list of pairs into an ordered set of leaves. \texttt{Successive-merge} is
    the procedure you must write, using \texttt{make-code-tree} to successively
    merge the smallest-weight elements of the set until there is only one
    element left, which is the desired Huffman tree. (This procedure is
    slightly tricky, but not really complicated. If you find yourself designing
    a complex procedure, then you are almost certainly doing something wrong.
    You can take significant advantage of the fact that we are using an ordered
    set representation.)
\end{quote}

\section{Exercise 2.70}
\begin{quote}
    The following eight-symbol alphabet with associated relative frequencies
    was designed to efficiently encode the lyrics of 1950s rock songs. (Note
    that the “symbols” of an “alphabet” need not be individual letters.)

    % There has to be a better way to change the typeface here.
    \begin{tabular}{>{\ttfamily}l>{\ttfamily}r>{\ttfamily}l>{\ttfamily}r}
        A    & 2 & NA  & 16 \\
        BOOM & 1 & SHA &  3 \\
        GET  & 2 & YIP &  9 \\
        JOB  & 2 & WAH &  1 \\
    \end{tabular}

    Use \texttt{generate-huffman-tree} (Exercise 2.69) to generate a
    corresponding Huffman tree, and use \texttt{encode} (Exercise 2.68) to
    encode the following message:

    \begin{allintypewriter}
Get a job
Sha na na na na na na na na

Get a job
Sha na na na na na na na na

Wah yip yip yip yip
yip yip yip yip yip
Sha boom
    \end{allintypewriter}

    How many bits are required for the encoding? What is the smallest number of
    bits that would be needed to encode this song if we used a fixed-length
    code for the eight-symbol alphabet?
\end{quote}

The encoded message is 87 bits. If a fixed-length code were used, there would
need to be at least 3 bits per symbol, because there are $8 = 2^3$ symbols.
The message has 36 symbols, so it would require 108 bits to encode in this way.

\section{Exercise 2.70}
\begin{quote}
    Suppose we have a Huffman tree for an alphabet of $n$ symbols, and that the
    relative frequencies of the symbols are $1,2,4,\ldots,2^{n−1}$. Sketch
    the tree for $n=5$; for $n=10$. In such a tree (for general $n$) how many
    bits are required to encode the most frequent symbol? The least frequent
    symbol?
\end{quote}

$n=5$:

\Tree [.{ABCDE 31}
    {E 16}
    [.{ABCD 15}
        {D 8}
        [.{ABC 7}
            {C 4}
            [.{AB 3}
                 {B 2}
                 {A 1} ]]]]

$n=10$:

\Tree [.{A-J 1024}
    {J 512}
    [.{A-I 511}
        {I 256}
        [.{A-H 255}
            {H 128}
            [.{A-G 127}
                {G 64}
                [.{A-F 63}
                    {F 32}
                    [.{ABCDE 31}
                        {E 16}
                        [.{ABCD 15}
                            {D 8}
                            [.{ABC 7}
                                {C 4}
                                [.{AB 3}
                                    {B 2}
                                    {A 1} ]]]]]]]]]

In general, in this kind of tree, the most frequent symbol can be encoded using
1 bit, and the least frequent symbol can be encoded using $n-1$ bits.

\section{Exercise 2.72}
\begin{quote}
    Consider the encoding procedure that you designed in Exercise 2.68. What is
    the order of growth in the number of steps needed to encode a symbol? Be
    sure to include the number of steps needed to search the symbol list at
    each node encountered. To answer this question in general is difficult.
    Consider the special case where the relative frequencies of the $n$ symbols
    are as described in Exercise 2.71, and give the order of growth (as a
    function of $n$) of the number of steps needed to encode the most frequent
    and least frequent symbols in the alphabet.
\end{quote}

Encoding the most frequent symbol takes $O(1)$ steps, assuming the tree was
constructed using \texttt{make-code-tree}, which, with this frequency
distribution, will always place the most frequent symbol on the left branch of
the root, by itself. For each sub-tree traversed (of which there will be one in
this case), \texttt{encode-symbol} performs a linear search of the symbols of
the left branch, possibly followed by a similar search of the right branch, if
the symbol was not in the left branch. In the case of the most frequent symbol,
it will be found in the left branch, all by itself, so the search will take
constant time. The recursion will then stop when it hits the leaf node
containing the most frequent symbol.

To encode the least frequent symbol, \texttt{encode-symbol} will have to
traverse the tree to its deepest leaf, taking the right branch at each
opportunity. At each level of recursion, of which there are $n-1$, a linear
search over the remaining symbols will take place. There will be one fewer
symbols at each subsequent level of the tree. The last level will be a leaf
node, so no search will take place, but a constant amount of work will be
performed. The number of steps to search every level is $\sum_{i=1}^{n}i =
\frac{n(n+1)}{2} = O(n^2)$.

\end{document}
