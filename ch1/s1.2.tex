\documentclass{article}
\author{Abraham Levkoy}
\title{SICP exercises, section 1.2}

\usepackage{mystyle}

\begin{document}
\maketitle

\section{Exercise 1.9}
\begin{quote}
    Each of the following two procedures defines a method for adding two
    positive integers in terms of the procedures \verb|inc|, which increments
    its argument by 1, and \verb|dec|, which decrements its argument by 1.
    \begin{lstlisting}
(define (+ a b)
  (if (= a 0)
      b
      (inc (+ (dec a) b))))

(define (+ a b)
  (if (= a 0)
      b
      (+ (dec a) (inc b))))
    \end{lstlisting}
    Using the substitution model, illustrate the process generated by each
    procedure in evaluating \verb|(+ 4 5)|. Are these processes iterative or
    recursive?
\end{quote}

The first procedure generates a recursive process:

\begin{minipage}{\linewidth}
\begin{verbatim}
(+ 4 5)
(inc (+ 3 5))
(inc (inc (+ 2 5)))
(inc (inc (inc (+ 1 5))))
(inc (inc (inc (inc (+ 0 5)))))
(inc (inc (inc (inc 5))))
(inc (inc (inc 6)))
(inc (inc 7))
(inc 8)
9
\end{verbatim}
\end{minipage}
% minipage does not seem to have a blank line after it like a normal paragraph
\vspace{\baselineskip}

The second procedure generates an iterative process:

\begin{minipage}{\linewidth}
\begin{verbatim}
(+ 4 5)
(+ 3 6)
(+ 2 7)
(+ 1 8)
(+ 0 9)
9
\end{verbatim}
\end{minipage}

\section{Exercise 1.10}
\begin{quote}
	The following procedure computes a mathematical function called Ackermann’s
	function.
    \begin{lstlisting}
(define (A x y)
  (cond ((= y 0) 0)
        ((= x 0) (* 2 y))
        ((= y 1) 2)
        (else (A (- x 1)
                 (A x (- y 1))))))
    \end{lstlisting}
    What are the values of the following expressions?

    \verb|(A 1 10)|\\
    \verb|(A 2 4)|\\
    \verb|(A 3 3)|\\
\end{quote}

\begin{minipage}{\linewidth}
\begin{verbatim}
(A 1 10)
(A 0 (A 1 9))
(A 0 (A 0 (A 1 8)))
(A 0 (A 0 (A 0 (A 1 7))))
(A 0 (A 0 (A 0 (A 0 (A 1 6)))))
(A 0 (A 0 (A 0 (A 0 (A 0 (A 1 5))))))
(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 1 4)))))))
(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 1 3))))))))
(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 1 2)))))))))
(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 1 1))))))))))
(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 2)))))))))
(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 4))))))))
(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 (A 0 8)))))))
(A 0 (A 0 (A 0 (A 0 (A 0 (A 0 16))))))
(A 0 (A 0 (A 0 (A 0 (A 0 32)))))
(A 0 (A 0 (A 0 (A 0 64))))
(A 0 (A 0 (A 0 128)))
(A 0 (A 0 256))
(A 0 512)
1024
\end{verbatim}
\end{minipage}
\vspace{\baselineskip}

\begin{minipage}{\linewidth}
\begin{verbatim}
(A 2 4)
(A 1 (A 2 3))
(A 1 (A 1 (A 2 2)))
(A 1 (A 1 (A 1 (A 2 1))))
(A 1 (A 1 (A 1 2)))
(A 1 (A 1 (A 0 (A 1 1))))
(A 1 (A 1 (A 0 2)))
(A 1 (A 1 4))
...
(A 1 16)
...
65536
\end{verbatim}
\end{minipage}
\vspace{\baselineskip}

\begin{minipage}{\linewidth}
\begin{verbatim}
(A 3 3)
(A 2 (A 3 2))
(A 2 (A 2 (A 3 1)))
(A 2 (A 2 2))
...
(A 2 4)
...
65536
\end{verbatim}
\end{minipage}
\vspace{\baselineskip}

\begin{quote}
    Consider the following procedures, where A is the procedure defined above:

    \begin{lstlisting}
(define (f n) (A 0 n))
(define (g n) (A 1 n))
(define (h n) (A 2 n))
(define (k n) (* 5 n n))
    \end{lstlisting}

    Give concise mathematical definitions for the functions computed by the
    procedures \verb|f|, \verb|g|, and \verb|h| for positive integer values of
    $n $. For example, \verb|(k n)| computes $5n^2$.
\end{quote}

\verb|(f n)| calculates $2n$.

\verb|(g n)| calculates $2^n$.

\verb|(h n)| calculates $2^{2^2}\ldots$ ($n$ levels of exponentiation).

\section{Exercise 1.11}
\begin{quote}
    A function $f$ is defined by the rule that $f(n) = n$ if $n < 3$ and
    $f(n) = f(n - 1) + 2f(n - 2) + 3f(n - 3)$ if $n \geq 3$. Write a procedure
    that computes $f$ by means of a recursive process. Write a procedure that
    computes $f$ by means of an iterative process.
\end{quote}

Recursive process:
\lstinputlisting[firstline=4,lastline=9]{ch1/ex1.11.scm}

Iterative process:
\lstinputlisting[firstline=12,lastline=25]{ch1/ex1.11.scm}

\section{Exercise 1.12}
\begin{quote}
	The following pattern of numbers is called Pascal’s triangle.

    \begin{tabular}{ccccccccc}
         & & & &1& & & & \\
         & & &1& &1& & & \\
         & &1& &2& &1& & \\
         &1& &3& &3& &1& \\
        1& &4& &6& &4& &1\\
        \multicolumn{9}{c}{$\cdots$}
    \end{tabular}

	The numbers at the edge of the triangle are all 1, and each number inside
	the triangle is the sum of the two numbers above it. Write a procedure that
	computes elements of Pascal's triangle by means of a recursive process.
\end{quote}

\parbox{\textwidth}{\lstinputlisting[firstline=2]{ch1/ex1.12.scm}}

\section{Exercise 1.13}
\begin{quote}
	Prove that $\textrm{Fib}(n)$ is the closest integer to $\phi^n/\sqrt{5}$,
	where $\phi = (1+\sqrt{5})/2$.  Hint: Let $\psi = (1-\sqrt{5})/2$. Use
	induction and the definition of the Fibonacci numbers (see 1.2.2) to prove
	that $\textrm{Fib}(n) = (\phi^n -\psi^n)/\sqrt{5}$.
\end{quote}

\begin{proof}
    Let $\phi = \frac{1+\sqrt{5}}{2}$. Let $\psi = \frac{1-\sqrt{5}}{2}$.

    Proof by induction. Base case:
    \begin{IEEEeqnarray*}{rCCCCCl}
        \frac{\phi^0-\psi^0}{\sqrt{5}} & = &&& 0 & = & \textrm{Fib}(0) \\
        \frac{\phi^1-\psi^1}{\sqrt{5}} & = &
            \frac{(1+\sqrt{5}) - (1-\sqrt{5})}{2\sqrt{5}} =
            \frac{2\sqrt{5}}{2\sqrt{5}} & = & 1 & = & \textrm{Fib}(1)
    \end{IEEEeqnarray*}

    Induction hypothesis:
    \begin{IEEEeqnarray*}{rCl}
        \textrm{Fib}(k) & = & \frac{\phi^k -\psi^k}{\sqrt{5}} \\
        \textrm{Fib}(k+1) & = &
            \frac{\phi^{k+1} - \psi^{k+1}}{\sqrt{5}} + \textrm{Fib}(k)
    \end{IEEEeqnarray*}

    \begin{IEEEeqnarray*}{rCl}
        \Rightarrow \textrm{Fib}(k+2) & = &
            \textrm{Fib}(k+1) + \textrm{Fib}(k) \\
        & = & \frac{\phi^{k+1}-\psi^{k+1}}{\sqrt{5}} +
            \frac{\phi^k-\psi^{k}}{\sqrt{5}} \\
        & = & \frac{\phi^k(\phi+1)-\psi^k(\psi+1)}{\sqrt{5}} \\
        & = & \frac{\frac{\phi^k(1+\sqrt{5}+2)}{2} -
            \frac{\psi^k(1-\sqrt{5}+2)}{2}}{\sqrt{5}} \\
        & = & \frac{\frac{\phi^k(2+2\sqrt{5}+4)}{4}-
            \frac{\psi^k(2-2\sqrt{5}+4)}{4}}{\sqrt{5}} \\
        & = & \frac{\phi^k\phi^2-\psi^k\psi^2}{\sqrt{5}} \\
        & = & \frac{\phi^{k+2}-\psi^{k+2}}{\sqrt{5}} \\
        \Rightarrow \textrm{Fib}(n) & = & \frac{\phi^n-\psi^n}{\sqrt{5}}
    \end{IEEEeqnarray*}

    $|\frac{\phi^n}{\sqrt{5}}| < 1$ for $n=1$. \\
    $\lim_{n \rightarrow \infty} |\frac{\phi^n}{\sqrt{5}}| \ll 1$. \\
    $\Rightarrow$ There is no integer closer to $\frac{\phi^n}{\sqrt{5}}$ than
        $\frac{\phi^n-\psi^n}{\sqrt{5}}$. \\
    $\Rightarrow \textrm{Fib}(n)$ is the closest integer to
        $\frac{\phi^n}{\sqrt{5}}$.
\end{proof}

\section{Exercise 1.13}
\begin{quote}
    Draw the tree illustrating the process generated by the
    \verb|count-change| procedure of 1.2.2 in making change for 11 cents. What
    are the orders of growth of the space and number of steps used by this
    process as the amount to be changed increases?
\end{quote}

\begin{allintypewriter}

\Tree [.{(count-change 11)}
    [.{(cc 11 5)}
        [.{(cc 11 4)}
            [.{(cc 11 3)}
                [.{(cc 11 2)} \edge[roof]; \textrm{See below} ]
                [.{(cc 1 2)}
                    [.{(cc 1 1)}
                        [.{(cc 0 0)} 1 ]
                        [.{(cc 1 0)} 0 ]]
                    [.{(cc -4 2)} 0 ]]]
            [.{(cc -14 4)} 0 ]]
        [.{(cc -39 5)} 0 ]]]

\Tree [.{(cc 11 2)}
    [.{(cc 11 1)} \edge[roof]; \textrm{See below} ]
    [.{(cc 6 2)}
        [.{(cc 6 1)}
            [.{(cc 6 0)} 0 ]
            [.{(cc 5 1)}
                [.{(cc 5 0)} 0 ]
                [.{(cc 4 1)} \edge[roof]; \textrm{See below} ]]]
        [.{(cc 1 2)}
            [.{(cc 1 1)}
                [.{(cc 0 0)} 1 ]
                [.{(cc 1 0)} 0 ]]
            [.{(cc -4 2)} 0 ]]]]

\Tree [.{(cc 4 1)}
    [.{(cc 4 0)} 0 ]
    [.{(cc 3 1)}
        [.{(cc 3 0)} 0 ]
        [.{(cc 2 1)}
            [.{(cc 2 0)} 0 ]
            [.{(cc 1 1)}
                [.{(cc 1 0)} 0 ]
                [.{(cc 0 1)} 1 ]]]]]

\Tree [.{(cc 11 1)}
    [.{(cc 11 0)} 0 ]
    [.{(cc 10 1)}
        [.{(cc 10 0)} 0 ]
        [.{(cc 9 1)}
            [.{(cc 9 0)} 0 ]
            [.{(cc 8 1)}
                [.{(cc 8 0)} 0 ]
                [.{(cc 7 1)}
                    [.{(cc 7 0)} 0 ]
                    [.{(cc 6 1)}
                        [.{(cc 6 0)} 0 ]
                        [.{(cc 5 1)} \edge[roof]; \textrm{See below} ]]]]]]]

\Tree [.{(cc 5 1)}
    [.{(cc 5 0)} 0 ]
    [.{(cc 4 1)}
        [.{(cc 4 0)} 0 ]
        [.{(cc 3 1)}
            [.{(cc 3 0)} 0 ]
            [.{(cc 2 1)}
                [.{(cc 2 0)} 0 ]
                [.{(cc 1 1)}
                    [.{(cc 1 0)} 0 ]
                    [.{(cc 0 1)} 1 ]]]]]]
\end{allintypewriter}

$\textrm{Space}(n) = \Theta(n)$. The longest branch will be the one where only
pennies are used, and the length with be $n+4$ (because there are 4 coins that
aren't pennies).

$\textrm{Time}(n)$: There will be five major branches on the tree:
\begin{enumerate}
    \item 1-coin branch (pennies)
    \item 2-coin branch (nickels)
    \item 3-coin branch (dimes)
    \item 4-coin branch (quarters)
    \item 5-coin branch (half dollars)
\end{enumerate}

Branch 1 has $\Theta(n)$ children, as the entire amount must be counted down one
penny at a time.

Branch 2 includes nickels, so it has $\Theta(n)$ pennies-only
branches for every additional nickel that can be subtracted from $n$. Thus,
Branch 2 is $\Theta((n/5)n)=\Theta(n^2/5)$.

Branch 3 additionally contains dimes, so it has a $\Theta(n^2/5)$ branch for
every additional dime that can be subtracted. Thus, branch 3 is
$\Theta((n/10)*n^2/5)=\Theta(n^3/50)$.

Branch 4 additionally contains quarters, so it has a $\Theta(n^3/50)$ branch for
every additional quarter that can be subtracted. Thus, branch 4 is
$\Theta((n/25)*n^3/50)=\Theta(n^4/1250)$.

Branch 5 additionally contains half dollars, so it has a $\Theta(n^4/1250)$
branch for every additional half dollar that can be subtracted. Thus, branch 5
is $\Theta((n/50)*n^4/1250)=\Theta(n^5/6250)$.

The overall $\textrm{Time}(n)$ is the sum of the 5 branches, but the
$\Theta(n^5/6250)$ branch overwhelms the others and simplifies to $\Theta(n^5)$.
Thus, $\textrm{Time}(n)=\Theta(n^5)$.

\section{Exercise 1.15}
\begin{quote}
    The sine of an angle (specified in radians) can be computed by making use
    of the approximation $\sin{x}\approx x$ if $x$ is sufficiently small, and
    the trigonometric identity
    \begin{equation*}
        \sin x = 3\sin\frac{x}{3}-4\sin^3\frac{x}{3}
    \end{equation*}
    to reduce the size of the argument of $\sin$. (For purposes of this
    exercise an angle is considered "sufficiently small" if its magnitude is
    not greater than 0.1 radians.) These ideas are incorporated in the
    following procedures:
    \begin{lstlisting}
(define (cube x) (* x x x))
(define (p x) (- (* 3 x) (* 4 (cube x))))
(define (sine angle)
(if (not (> (abs angle) 0.1))
   angle
   (p (sine (/ angle 3.0)))))
    \end{lstlisting}
    \begin{enumerate}
        \item How many times is the procedure \verb|p| applied when
            \texttt{(sine 12.15)} is evaluated?
        \item What is the order of growth in space and number of steps (as a
            function of $a$) used by the process generated by the sine
            procedure when \texttt{(sine a)} is evaluated?
    \end{enumerate}
\end{quote}

There is one recursive call of \texttt{sine} per reduction in \texttt{angle},
and \texttt{angle} is divided by 3 each time \texttt{sine} is called
recursively. For \texttt{(sine 12.15)}, the sequence of calls (with the first
call the last to return) is \texttt{(sine 12.15)} $\rightarrow$ \texttt{(sine
4.05)} $\rightarrow$ \texttt{(sine 1.35)} $\rightarrow$ \texttt{(sine .45)}
$\rightarrow$ \texttt{(sine .15)} $\rightarrow$ \texttt{(sine .05)}. That's a
total of 6 evaluations of \texttt{sine}. In all but the last one, procedure
\texttt{p} is applied, so \texttt{p} is evaluated 5 times.

Input \texttt{a} will be divided by 3 until it is less than 0.1, so multiplying
\texttt{a} by 3 will increase by 1 the number of steps taken.
$\textrm{Time}(a)=\Theta(\log a)$.

In the recursive case, the recursive call is not the last step taken, so it
cannot be evaluated prior to further reduction, and all steps will be on the
stack before the process completes. $\textrm{Space}(a)=\Theta(\log a)$.

\section{Exercise 1.16}
\begin{quote}
	Design a procedure that evolves an iterative exponentiation process that
	uses successive squaring and uses a logarithmic number of steps, as does
	\texttt{fast-expt}. (Hint: Using the observation that
	$(b^{n/2})^2=(b^2)^{n/2}$, keep, along with the exponent $n$ and the base
	$b$ , an additional state variable $a$ , and define the state transformation
	in such a way that the product $ab^n$ is unchanged from state to state. At
	the beginning of the process $a$ is taken to be 1, and the answer is given
	by the value of $a$ at the end of the process. In general, the technique of
	defining an invariant quantity that remains unchanged from state to state
	is a powerful way to think about the design of iterative algorithms.)
\end{quote}

\lstinputlisting[firstline=2]{ch1/ex1.16.scm}

\section{Exercise 1.17}
\begin{quote}
    The exponentiation algorithms in this section are based on performing
    exponentiation by means of repeated multiplication. In a similar way, one
    can perform integer multiplication by means of repeated addition. The
    following multiplication procedure (in which it is assumed that our
    language can only add, not multiply) is analogous to the expt procedure:
    \begin{lstlisting}
(define (* a b)
  (if (= b 0)
      0
      (+ a (* a (- b 1)))))
    \end{lstlisting}
    This algorithm takes a number of steps that is linear in \texttt{b}. Now
    suppose we include, together with addition, operations \texttt{double},
    which doubles an integer, and \texttt{halve}, which divides an (even)
    integer by 2. Using these, design a multiplication procedure analogous to
    \texttt{fast-expt} that uses a logarithmic number of steps.
\end{quote}

\lstinputlisting[firstline=4]{ch1/ex1.17.scm}

\section{Exercise 1.18}
\begin{quote}
    Using the results of Exercise 1.16 and Exercise 1.17, devise a procedure
    that generates an iterative process for multiplying two integers in terms
    of adding, doubling, and halving and uses a logarithmic number of steps.
\end{quote}

\lstinputlisting[firstline=11]{ch1/ex1.18.scm}

\section{Exercise 1.19}
\begin{quote}
    There is a clever algorithm for computing the Fibonacci numbers in a
    logarithmic number of steps. Recall the transformation of the state
    variables $a$ and $b$ in the \texttt{fib-iter} process of 1.2.2:
    $a \leftarrow a+b$ and $b \leftarrow a$. Call this transformation $T$, and
    observe that applying $T$ over and over again $n$ times, starting with 1
    and 0, produces the pair $\textrm{Fib}(n+1)$ and $\textrm{Fib}(n)$. In
    other words, the Fibonacci numbers are produced by applying $T^n$, the
    $n^\textrm{th}$ power of the transformation $T$, starting with the pair
    $(1,0)$. Now consider $T$ to be the special case of $p=0$ and $q=1$ in a
    family of transformations $T_{pq}$, where $T_{pq}$ transforms the pair
    $(a,b)$ according to $a \leftarrow bq+aq+ap$ and $b \leftarrow bp+aq$. Show
    that if we apply such a transformation $T_{pq}$ twice, the effect is the
    same as using a single transformation $T_{p'q'}$ of the same form, and
    compute $p'$ and $q'$ in terms of $p$ and $q$. This gives us an explicit
    way to square these transformations, and thus we can compute $T^n$ using
    successive squaring, as in the \texttt{fast-expt} procedure. Put this all
    together to complete the following procedure, which runs in a logarithmic
    number of steps:
	\begin{lstlisting}
(define (fib n)
  (fib-iter 1 0 0 1 n))

(define (fib-iter a b p q count)
  (cond ((= count 0)
         b)
        ((even? count)
         (fib-iter a
                   b
                   <??>  ;compute p'
                   <??>  ;compute q'
                   (/ count 2)))
        (else
         (fib-iter (+ (* b q)
                      (* a q)
                      (* a p))
                   (+ (* b p)
                      (* a q))
                   p
                   q
                   (- count 1)))))
	\end{lstlisting}
\end{quote}

\begin{IEEEeqnarray*}{RL}
    T(a,b): & a \leftarrow a + b \\
    & b \leftarrow a \\
    T_{pq}(a,b): & a \leftarrow bq + aq + ap \\
    & b \leftarrow bp + aq \\
    T_{pq}^2(a,b): & a_1 \leftarrow bq + aq + ap \\
    & b_1 \leftarrow bp + aq \\
    & a \leftarrow b_1q + a_1q + a_1p \\
    & b \leftarrow b_1p + a_1q \\
    \Rightarrow & a \leftarrow (bp + aq)q + (bq + aq + ap)(p + q) \\
    & b \leftarrow (bp + aq)p + (bq + aq + ap)q \\
    \Rightarrow &
        a \leftarrow bpq + aq^2 + bpq + apq + ap^2 + bq^2 + aq^2 + apq \\
    & b \leftarrow bp^2 + apq + bp^2 + aq^2 + apq \\
    \Rightarrow & a \leftarrow 2bpq + 2aq^2 + 2apq + ap^2 + bq^2 \\
    & b \leftarrow bp^2 + 2apq + bq^2 + aq^2 \\
    \Rightarrow & a \leftarrow (2pq + q^2)b + (2q^2 + 2pq +p^2)a \\
    & b \leftarrow (p^2 + q^2)b + (2pq + q^2)a \\
    \Rightarrow & a \leftarrow b(2pq + q^2) + a(2pq + q^2) + a(p^2 + q^2) \\
    & b \leftarrow b(p^2 + q^2) + a(2pq + q^2)
\end{IEEEeqnarray*}

Let $p' = p^2 + q^2$ and $q' = 2pq + q^2$. $T_{p'q'} = T_{pq}^2(a,b)$.

\section{Exercise 1.20}
\begin{quote}
    The process that a procedure generates is of course dependent on the rules
    used by the interpreter. As an example, consider the iterative \texttt{gcd}
    procedure given above. Suppose we were to interpret this procedure using
    normal-order evaluation, as discussed in 1.1.5. (The
    normal-order-evaluation rule for \texttt{if} is described in Exercise 1.5.)
    Using the substitution method (for normal order), illustrate the process
    generated in evaluating \texttt{(gcd 206 40)} and indicate the remainder
    operations that are actually performed. How many remainder operations are
    actually performed in the normal-order evaluation of \texttt{(gcd 206 40)}?
    In the applicative-order evaluation?
\end{quote}

\subsection{Normal order}
In normal order, \texttt{a} is not evaluated unless \texttt{b} is 0. Therefore,
during expansion, remainder operations in the \texttt{b} argument of each
recursive call are performed, but \texttt{remainder} operations in \texttt{a}
are not performed until the final sequence of reductions.

\begin{enumerate}
    \item \lstinline|(gcd 206 40)|\\
        $a=206$\\
        $b=40$

    \item
        \begin{lstlisting}
(gcd 40
     (remainder 206 40))
        \end{lstlisting}
        $a=40$\\
        $b=\texttt{(remainder 206 40)}$\\
        1 \texttt{remainder} operation performed.

    \item
        \begin{lstlisting}
(gcd (remainder 206 40)
     (remainder 40
                (remainder 206 40)))
        \end{lstlisting}
        $a=\texttt{(remainder 206 40)}=6$\\
        $b=\texttt{(remainder 40 (remainder 206 40))}=4$\\
        2 \texttt{remainder} operations performed.

    \item
        \begin{lstlisting}
(gcd (remainder 40
                (remainder 206 40))
     (remainder (remainder 206 40)
                (remainder 40
                           (remainder 206 40))))
        \end{lstlisting}
        $a=\texttt{(remainder 40 (remainder 206 40))}=4$\\
        $b=$
        \begin{lstlisting}
(remainder (remainder 206 40)
           (remainder 40
                      (remainder 206 40)))
        \end{lstlisting}
        $=2$\\
        4 \texttt{remainder} operations performed.

    \item
        \begin{lstlisting}
(gcd (remainder (remainder 206 40)
        (remainder 40
           (remainder 206 40)))
     (remainder (remainder 40
           (remainder 206 40))
        (remainder (remainder 206 40)
           (remainder 40
              (remainder 206 40)))))
        \end{lstlisting}
        $a=$
        \begin{lstlisting}
(remainder (remainder 206 40)
           (remainder 40
                      (remainder 206 40)))
        \end{lstlisting}
        $=2$\\

        $b=$\\
        \begin{lstlisting}
(remainder (remainder 40 (remainder 206 40))
   (remainder (remainder 206 40)
      (remainder 40
         (remainder 206 40))))
        \end{lstlisting}
        $=0$, so \texttt{gcd} returns \texttt{a} instead of recursing.\\
        7 \texttt{remainder} operations performed.

    \item
        \begin{lstlisting}
(remainder (remainder 206 40)
           (remainder 40
                      (remainder 206 40)))
        \end{lstlisting}
        All \texttt{remainder} operations in reduction are performed.

    \item \lstinline|(remainder (remainder 206 40) (remainder 40 6))|
    \item \lstinline|(remainder 6 (remainder 40 6))|
    \item \lstinline|(remainder 6 4)|
    \item \lstinline|2|\\
        4 \texttt{remainder} operations performed during reduction. 11 total
        \texttt{remainder} operations are performed.
\end{enumerate}

\subsection{Applicative order}
\begin{lstlisting}
(gcd 206 40)
(gcd 40 (remainder 206 40))
(gcd 40 6)
(gcd 6 (remainder 40 6))
(gcd 6 4)
(gcd 4 (remainder 6 4))
(gcd 4 2)
(gcd 2 (remainder 4 2))
(gcd 2 0)
2
\end{lstlisting}
\texttt{remainder} is evaluated whenver it appears, 4 times in total

\section{Exercise 1.21}
\begin{quote}
    Use the smallest-divisor procedure to find the smallest divisor of each of
    the following numbers: 199, 1999, 19999.
\end{quote}

The smallest divisor of 199 is 199. The smallest divisor of 1999 is 1999. The
smallest divisor of 19999 is 7.

\section{Exercise 1.22}
\begin{quote}
    Most Lisp implementations include a primitive called \texttt{runtime} that
    returns an integer that specifies the amount of time the system has been
    running (measured, for example, in microseconds). The following
    \texttt{timed-prime-test} procedure, when called with an integer $n$,
    prints $n$ and checks to see if $n$ is prime. If $n$ is prime, the
    procedure prints three asterisks followed by the amount of time used in
    performing the test.

    \begin{lstlisting}
(define (timed-prime-test n)
  (newline)
  (display n)
  (start-prime-test n (runtime)))

(define (start-prime-test n start-time)
  (if (prime? n)
      (report-prime (- (runtime)
                       start-time))))

(define (report-prime elapsed-time)
  (display " *** ")
  (display elapsed-time))
    \end{lstlisting}

    Using this procedure, write a procedure \texttt{search-for-primes} that
    checks the primality of consecutive odd integers in a specified range. Use
    your procedure to find the three smallest primes larger than 1000; larger
    than 10,000; larger than 100,000; larger than 1,000,000. Note the time
    needed to test each prime. Since the testing algorithm has order of growth
    of $\Theta(\sqrt{n})$, you should expect that testing for primes around
    10,000 should take about $\sqrt{10}$ times as long as testing for primes
    around 1000. Do your timing data bear this out? How well do the data for
    100,000 and 1,000,000 support the $\Theta(\sqrt{n})$ prediction? Is your
    result compatible with the notion that programs on your machine run in time
    proportional to the number of steps required for the computation?
\end{quote}

\lstinputlisting[firstline=35]{ch1/ex1.22.scm}

3 smallest primes larger than
\begin{tabular}{r|ccc}
    Prime order&\multicolumn{3}{c}{Primes/Times}\\
    1,000&1,009&1,013&1,019\\
    &0 s&0 s&0 s\\
    \\
    10,000&10,007&10,009&10,037\\
    &.01 s&0 s&0 s\\
    \\
    100,000&100,003&100,019&100,043\\
    &.01 s&0 s&0 s\\
    \\
    1,000,000&1,000,003&1,000,033&1,000,037\\
    &0 s&0 s&.01 s\\
\end{tabular}

Perhaps due to 20 year gap between latest edition of \emph{SICP} and present,
all of the specified prime computations took approximately zero seconds of
process time. There is not an obvious way to obtain the time with higher
granularity than ticks, which appear to be 1 ms. In order to observe the
expected asymptotic complexity behavior, I will have to use much larger
numbers.

\begin{tabular}{r|cccc}
    Prime order&\multicolumn{3}{c}{Primes/Times}&Multiple of next lower\\
    1,000,000,000&1,000,000,007&1,000,000,009&1,000,000,021\\
    &.090 s&.060 s&.049 s&---\\
    10,000,000,000&10,000,000,019&10,000,000,033&10,000,000,061\\
    &.160 s&.150 s&.150 s&2.50\\
    100,000,000,000&100,000,000,003&100,000,000,019&100,000,000,057\\
    &.550 s&.500 s&.520 s&3.47\\
    1,000,000,000,000&1,000,000,000,039&1,000,000,000,061&1,000,000,000,063\\
    &1.60 s&1.61 s&1.59 s&3.08\\
\end{tabular}

Increasing $n$ by a multiple of 10 increases runtime by a multiple of
approximately $\sqrt{10}\approx3.16$, supporting a $\Theta(\sqrt{n})$ time
complexity prediction. These results support the notion that programs on my
machine run in time proportional to the number of steps required for
computation. As usual, the actual increase in runtime gets closer to the
predicted increase as the size of the input increases.

\section{Exercise 1.23}
\begin{quote}
    The smallest-divisor procedure shown at the start of this section does lots
    of needless testing: After it checks to see if the number is divisible by 2
    there is no point in checking to see if it is divisible by any larger even
    numbers. This suggests that the values used for test-divisor should not be
    2, 3, 4, 5, 6, \ldots, but rather 2, 3, 5, 7, 9, \ldots. To implement this
    change, define a procedure next that returns 3 if its input is equal to 2
    and otherwise returns its input plus 2. Modify the
    \texttt{smallest-divisor} procedure to use \texttt{(next test-divisor)}
    instead of \texttt{(+ test-divisor 1)}. With \texttt{timed-prime-test}
    incorporating this modified version of \texttt{smallest-divisor}, run the
    test for each of the 12 primes found in Exercise 1.22. Since this
    modification halves the number of test steps, you should expect it to run
    about twice as fast. Is this expectation confirmed? If not, what is the
    observed ratio of the speeds of the two algorithms, and how do you explain
    the fact that it is different from 2?
\end{quote}

\begin{tabular}{r|ccccc}
    Prime order&\multicolumn{3}{c}{Primes/Times}&Multiple of old version\\
    1,000,000,000&1,000,000,007&1,000,000,009&1,000,000,021\\
    &.040 s&.030 s&.030 s&.50\\
    10,000,000,000&10,000,000,019&10,000,000,033&10,000,000,061\\
    &.10 s&.10 s&.10 s&.65\\
    100,000,000,000&100,000,000,003&100,000,000,019&100,000,000,057\\
    &.35 s&.30 s&.33 s&.62\\
    1,000,000,000,000&1,000,000,000,039&1,000,000,000,061&1,000,000,000,063\\
    &1.0 s&1.0 s&1.0 s&.63\\
\end{tabular}

We might expect a reduction in runtime of about 50\%, but we see a somewhere
around 35\%. One explanation for this could be that we have replaced our
unconditional add operation for each iteration with a conditional add that
tests the value of its input.

\section{Exercise 1.24}
\begin{quote}
    Modify the \texttt{timed-prime-test} procedure of Exercise 1.22 to use
    \texttt{fast-prime?} (the Fermat method), and test each of the 12 primes
    you found in that exercise. Since the Fermat test has $\Theta(\log{n})$
    growth, how would you expect the time to test primes near 1,000,000 to
    compare with the time needed to test primes near 1000? Do your data bear
    this out? Can you explain any discrepancy you find?
\end{quote}

\lstinputlisting[firstline=27,lastline=39]{ch1/ex1.24.scm}

Due to the increased efficiency of the Fermat method, all of the previously
identified primes are now identified in an amount of time below the granularity
of the \texttt{runtime} procedure. In order to get comparable time results, I
have increased the number of iterations per input from 5 to 5000.

\begin{tabular}{r|cccc}
    Prime order&\multicolumn{3}{c}{Primes/Times}\\
    1,000,000,000&1,000,000,007&1,000,000,009&1,000,000,021\\
    &.45 s&.44 s&.44 s\\
    10,000,000,000&10,000,000,019&10,000,000,033&10,000,000,061\\
    &.53 s&.51 s&.54 s\\
    100,000,000,000&100,000,000,003&100,000,000,019&100,000,000,057\\
    &.56 s&.58 s&.58 s\\
    1,000,000,000,000&1,000,000,000,039&1,000,000,000,061&1,000,000,000,063\\
    &.56 s&.59 s&.61 s\\
\end{tabular}

I would expect each multiplication of the input by 10 to result in an
incremental increase in runtime. If you squint, there is an increase of
\emph{roughly} .06 s for each increase in the order of magnitude. If I test
primes on the order of 100,000, such as 100,019, the runtime is approximately
.23 s or about half of the runtimes for the primes on the order of
100,000,000,000. This shows an approximate doubling of the runtime when the
input is squared (order-of-magnitude approximation), consistent with a runtime
complexity of $\Theta(\log{n})$.

\end{document}
